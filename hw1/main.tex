\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=2cm]{geometry}

%% Useful packages
\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[colorinlistoftodos]{todonotes}
% \usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{enumerate}
\usepackage{float}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{tikzscale}
\usetikzlibrary{shapes.geometric, arrows}
\tikzset{
    vertex/.style={circle,draw,minimum size=1.5em},
    edge/.style={->,> = latex'}
}
\tikzstyle{triger} = [circle, minimum width=2cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{process} = [rectangle, minimum width=1cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{decision} = [diamond, minimum width=2cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{block} = [rectangle, minimum width=3cm, minimum height=3cm, text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]

\title{HW1}
\author{Kevin Chang}

\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{property}{Property}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{suspect}{Suspect}[section]
\newtheorem{example}{Example}
\newtheorem{lemma}[theorem]{Lemma}

\graphicspath{ {./images/} }

\begin{document}
\maketitle

\section{}
Let $\xi_i$ be a sequence of independent random variables and $X_n$ a function of the first $n$ elements of the sequence.
Define
$$\mathbb{E}_k [X_n] := \mathbb{E}[X_n |\xi_1 , \dots, \xi_k ].$$

\paragraph{1.1}
Show that
$$\mathbb{E}[(X_n - \mathbb{E}[X_n])^2]= \sum_{k=1}^n \mathbb{E}[(\mathbb{E}_k [X_n ] - \mathbb{E}_{k-1} [X_n ])^2].$$

\begin{itemize}
    \item \emph{Answer:}
    \item Observe that $\mathbb{E}_n[X_n] = X_n$ and $\mathbb{E}_0[X_n] = \mathbb{E}[X_n]$.
    \item Then,
    \begin{align*}
        \mathbb{E}\!\left[(X_n - \mathbb{E}[X_n])^2\right] 
        &= \mathbb{E}\!\left[\left(\sum_{k=1}^n \big(\mathbb{E}_k[X_n] - \mathbb{E}_{k-1}[X_n]\big)\right)^2\right] \\
        &= \mathbb{E}\!\Big[\sum_{k=1}^n \big(\mathbb{E}_k[X_n] - \mathbb{E}_{k-1}[X_n]\big)^2 \\
        & \quad + 2 \sum_{1 \leq k_1 < k_2 \leq n} \big(\mathbb{E}_{k_1}[X_n] - \mathbb{E}_{k_1-1}[X_n]\big)\big(\mathbb{E}_{k_2}[X_n] - \mathbb{E}_{k_2-1}[X_n]\big)\Big].
    \end{align*}
    \item For the cross terms: since
    \[
        \mathbb{E}\!\left[\mathbb{E}_{k_1}[X_n] - \mathbb{E}_{k_1-1}[X_n] \,\right] = 0,
    \]
    it follows that
    \[
        \mathbb{E}\!\left[\big(\mathbb{E}_{k_1}[X_n] - \mathbb{E}_{k_1-1}[X_n]\big)\big(\mathbb{E}_{k_2}[X_n] - \mathbb{E}_{k_2-1}[X_n]\big)\right] = 0
        \qquad \text{for all } k_1 < k_2.
    \]
    \item Hence,
    \[
        \mathbb{E}\!\left[(X_n - \mathbb{E}[X_n])^2\right]
        = \sum_{k=1}^n \mathbb{E}\!\left[\big(\mathbb{E}_k[X_n] - \mathbb{E}_{k-1}[X_n]\big)^2\right].
    \]
\end{itemize}

\paragraph{1.2}
Suppose there is a constant B such that $|\mathbb{E}_k [X_n ] - \mathbb{E}_{k-1} [X_n ]| \leq B$ with probability 1.
Prove that
$$\lim_{n \rightarrow \infty} \frac{1}{n^2} \mathbb{E}[(X_n - \mathbb{E}[X_n ])^2 ] = 0$$

\begin{itemize}
    \item \emph{Answer:}
    \begin{align*}
        \lim_{n \rightarrow \infty}\mathbb{E}\!\left[(X_n - \mathbb{E}[X_n])^2\right]
        &= \lim_{n \rightarrow \infty}\sum_{k=1}^n \mathbb{E}[(\mathbb{E}_k [X_n ] - \mathbb{E}_{k-1} [X_n ])^2] \\
        \leq \lim_{n \rightarrow \infty}\frac{nB^2}{n^2} = 0 \\
    \end{align*}
\end{itemize}

\paragraph{1.3}
Let $Z_i$ be a sequence of independent, identically distributed random variables that take values in $[0, 1]$.
Let $\mu = \mathbb{E}[Z_1 ]$ Use the above parts to prove
$$\lim_{n \rightarrow \infty} \mathbb{E}\left[\left(\frac{1}{n} \sum_{i=1}^n Z_i - \mu\right)^2\right]  = 0$$

\begin{itemize}
    \item $X_n =  \sum_{i = 1}^n Z_i - n \mathbb{E}[Z_i]$
    $$\mathbb{E}[(X_n - \mathbb{E}[X_n])^2] = \mathbb{E}[(X_n)^2]= \sum_{k=1}^n \mathbb{E}[(\mathbb{E}_k [X_n ] - \mathbb{E}_{k-1} [X_n ])^2]=0$$
\end{itemize}
\section{}
An ordering of the the set $\{1, . . . , T \}$ is a sequence of integers $\{\sigma_1 , . . . \sigma_T \}$ where every number between $1$ and $T$ appears exactly once.
A sequence of random variables $x_1 , . . . , x_T$ is called exchangeable if the distribution of $x_1 , . . . , x_T$ is equal to the distribution $x_{\sigma_1}, . . . , x_{\sigma_T}$ for any ordering of the indices.
\paragraph{2.1}
Show that a sequence is exchangeable if and only if for any $i$ and $j$, the distribution is the same when elements $x_i$ and $x_j$ are swapped.
\begin{itemize}
    \item[$(\Rightarrow)$] If $(x_1,\dots,x_T)$ is exchangeable, then for any indices $i$ and $j$, swapping $x_i$ and $x_j$ does not change the joint distribution.
    \item[$(\Leftarrow)$] Conversely, if every pair $(x_i, x_j)$ can be swapped without changing the joint distribution, then the sequences $(x_1,\dots,x_T)$ and $(x_{\sigma_1},\dots,x_{\sigma_T})$ have the same distribution. Hence, the sequence is exchangeable.
\end{itemize}

\paragraph{2.2}
Show that any iid sequence is exchangeable.

If $x_1,\dots,x_T$ are iid then they share the same distribution.
\paragraph{2.3}
Show that for any indices $i$ and $j$, $E[x_j ] = E[x_i ]$.

By 2.1, swapping $i$ and $j$ leaves the joint distribution unchanged.
Thus $\mathbb{E}[x_i]=\mathbb{E}[x_j]$ for all $i,j$.
\paragraph{2.4}
Letâ€™s suppose we want to predict values in an exchangeable sequence.
Compute the expected mean squared error of using the mean of the first $T - 1$ elements of the sequence to predict the last.
That is, compute
$$\mathbb{E}\left[\left(x_T - \frac{1}{T-1}\sum_{t=1}^{T-1}x_t\right)^2\right].$$
when $x_t$ is an exchangable sequence.

\begin{itemize}
    \item $\{X_i\}_{i=1}^\infty$ are exchangable random variables
    \item $Z_T = (T-1) X_T - \sum_{t=1}^{T-1} \mathbb{E}[X_i]$
    \item $\mathbb{E}[Z_{T+1}| Z_0, \dots, Z_T]$
                        $= \mathbb{E}[Z_T + X_{T+1} - \mathbb{E}[X_T]| Z_0, \dots, Z_T] = Z_T$
    \item By martingales: $\mathbb{E}[Z_T] = \mathbb{E}[Z_1] = 0$
\end{itemize}

\section{}
A symmetric, $n \times n$ matrix $A$ is called positive definite if for all nonzero vectors $x \in \mathbb{R}^n , x^T Ax > 0$.
We say a function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ has positive curvature if its Hessian matrix, $\nabla^2 f(x)$ is positive definite for all $x$.
\paragraph{3.1}
Suppose $f$ has positive curvature. For any vectors $x$ and $z$, we can define a function that maps $\mathbb{R}$ to $\mathbb{R}$ by $g(t) := f (x + tz)$.
Show the second derivative of $g$ is positive for all $t$.
\begin{itemize}
    \item assume $\nabla^2 f(t) = H$
    \item $\nabla^2 f(x + tz) = z^2H$ (positive define)
\end{itemize}
\paragraph{3.2}
Suppose f : $\mathbb{R}^n \rightarrow \mathbb{R}$ has positive curvature.
Let $M$ be a $n \times d$ dimensional full-rank matrix and $v$ be an $n$-dimensional vector.
Define $h : \mathbb{R}^d \rightarrow \mathbb{R}$ by $g(u) = f (M u + v).$
Show that $g$ has positive curvature.

\begin{itemize}
    \item \emph{Answer}
    \item $\nabla^2 f(Mu+v) = M^T\nabla^2f(Mu+v)M$ (positive define)
\end{itemize}
\paragraph{3.3}
Suppose $f$ has positive curvature and $h$ is a real valued function that is monotonically increasing and
whose derivative is monotonically strictly increasing. Show that the composition function $h \circ f$ , where

$h \circ f (x) = h(f (x))$, has positive curvature.
$$\nabla (h\circ f)(x) = h'(f(x))\, \nabla f(x).$$
Differentiating again,
$$\nabla^2 (h\circ f)(x) = h''(f(x))\, \nabla f(x)\,\nabla f(x)^\top \;+\; h'(f(x))\, \nabla^2 f(x).$$

Now check positive semidefiniteness: for any $v \in \mathbb{R}^n$,
\[
v^\top \nabla^2 (h\circ f)(x)\, v
= h''(f(x))\, (v^\top \nabla f(x))^2 \;+\; h'(f(x))\, v^\top \nabla^2 f(x)\, v.
\]
Since $h''(f(x)) \ge 0$, $(v^\top \nabla f(x))^2 \ge 0$, $h'(f(x)) \ge 0$, and $v^\top \nabla^2 f(x)\, v > 0$ (because $f$ is convex), it follows that
\[
v^\top \nabla^2 (h\circ f)(x)\, v \; >\; 0 \quad \text{for all } v.
\]

Thus $\nabla^2 (h\circ f)(x)$ is positive semidefinite for all $x$, and hence $h\circ f$ is convex (has positive curvature).
\qed

\section{}
Let $A$ be an $n \times n$ positive definite matrix and let $v$ be a $n$-vector with Euclidean norm 1.
Define $x_0 = v$ and, for integers $k \geq 1$, set $x_k = Ax_{k-1}.$
\paragraph{4.1}
Suppose all of the eigenvalues of $A$ are less than 1.
Show that $||x_k || \leq \beta^k$ for some number $\beta \in [0, 1]$.
Here
$$||z|| = \left( \sum_{k=1}^n z_k^2 \right)^{1/2}.$$

Assume $A = Q \Omega Q^\top$, where $Q$ is orthogonal and $\Omega = \operatorname{diag}(\lambda_1,\dots,\lambda_n)$ contains the eigenvalues of $A$.
For any vector $v$,
\[
\|x_k\| = \|A^k v\| = \|Q \Omega^k Q^\top v\| = \|\Omega^k (Q^\top v)\|.
\]
Since $\|\Omega^k y\|^2 = \sum_{i=1}^n (\lambda_i^k y_i)^2$ and $|\lambda_i| < 1$, it follows that
\[
\|x_k\| \;\le\; (\max_i |\lambda_i|)^k \,\|v\|.
\]
Setting $\beta := \max_i |\lambda_i| \in [0,1)$, we obtain
\[
\|x_k\| \le \|v\|\, \beta^k.
\]

\paragraph{4.2}
Suppose one of the eigenvalues of $A$ is equal to 2.
Can the sequence $||x_k||$ converge to zero?
Explain your answer.

Let $v$ have a nonzero component in the eigenspace corresponding to $\lambda=2$. Then
\[
x_k = A^k v
\]
contains a term proportional to $2^k$, so $\|x_k\|$ grows without bound and cannot converge to $0$.
The only exception is if the initial vecto


\section{}
Please submit a short (less than 250 words) final project proposal.
The proposal should describe the project idea as well its connections to the course material.
This proposal is flexible and can change as the semester continues.
Choose a topic that is of interest to you and related to your research goals.
You will submit something about the project along with every problem set, so you need to have an active, but potentially changing, project.
The proposal should have the format and flow of the abstract of a conference or journal paper.
This project should have the potential to gather real data to solve some prediction problem.
Please state your data source and the prediction problem you are interested in studying.
Teams of 1, 2 or 3 are allowed for these projects. Each project team should submit their information here so we know who is working together on what. All members of a team should include the same proposal in their homework submission.

\end{document}

